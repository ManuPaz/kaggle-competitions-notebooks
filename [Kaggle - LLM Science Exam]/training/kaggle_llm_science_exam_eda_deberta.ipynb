{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c774b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61fc6502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manue\\anaconda3\\envs\\kagle\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.constants import (\n",
    "    DATA_PATH,\n",
    "    USE_SAMPLE,\n",
    "    FREEZE_EMBEDDINGS,\n",
    "    FREEZE_LAYERS,\n",
    "    MODEL_NAME,\n",
    "    MODEL_DIR,\n",
    "    CHECKPOINT_DIR,\n",
    "    OUTPUT_DIR,\n",
    "    SEED,\n",
    "    N_SPLITS,\n",
    "    REMOVE_COLS,\n",
    "    FOLDING,\n",
    ")\n",
    "from src.params import (\n",
    "    MODEL_NAME,MAX_INPUT_TRAIN,MAX_INPUT_VAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c47aea",
   "metadata": {
    "id": "81c47aea"
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "# Statistics & Mathematics\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro, skew\n",
    "import math\n",
    "# RFECV for feature selection\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Machine Learning Pipeline & process\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Preprocessing data\n",
    "from sklearn.preprocessing import (\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    QuantileTransformer,\n",
    "    FunctionTransformer,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# Model Selection for Cross Validation\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "# Machine Learning metrics\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_error,\n",
    "    cohen_kappa_score,\n",
    "    make_scorer,\n",
    ")\n",
    "# ML regressors\n",
    "from sklearn.linear_model import HuberRegressor, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.ensemble import (\n",
    "    HistGradientBoostingRegressor,\n",
    "    StackingRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "# ML classifiers\n",
    "from sklearn.ensemble import (\n",
    "    HistGradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "# Clustering model\n",
    "from sklearn.cluster import KMeans\n",
    "# Randomizer\n",
    "import random\n",
    "# Encoder of categorical variables\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "# Importing HuggingFace's Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMultipleChoice,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from transformers.tokenization_utils_base import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PaddingStrategy,\n",
    ")\n",
    "# PyTorch\n",
    "import torch\n",
    "# Hiding warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca96653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import preprocess_wrapper, DataCollatorForMultipleChoice,setup_device,create_option_mappings\n",
    "from src.metrics import compute_metrics, competition_score, predictions_to_map_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf69dbe",
   "metadata": {
    "id": "eaf69dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "device= setup_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9392de",
   "metadata": {
    "id": "4e9392de"
   },
   "outputs": [],
   "source": [
    "# synthetic data generated and used by severale users in the competition\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, \"train_with_context2.csv\"))  # Loading data\n",
    "# Concatenating original dataframe to extra dataframes\n",
    "augmented_df = pd.read_csv(os.path.join(DATA_PATH, \"all_12_with_context2.csv\"))\n",
    "if \"id\" in df.columns:\n",
    "    df = df.drop(\"id\", axis=1)  # Dropping 'Id' columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98fb58c4",
   "metadata": {
    "id": "98fb58c4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>context</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In relation to Eunice Fay McKenzie's career, w...</td>\n",
       "      <td>Eunice Fay McKenzie (February 19, 1918 – April...</td>\n",
       "      <td>McKenzie showcased her singing talents in nume...</td>\n",
       "      <td>McKenzie is primarily remembered for her starr...</td>\n",
       "      <td>McKenzie gained recognition for her role as a ...</td>\n",
       "      <td>McKenzie's collaborations with director Blake ...</td>\n",
       "      <td>McKenzie's successful career in sound films co...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does Modified Newtonian Dynamics (MOND) im...</td>\n",
       "      <td>The presence of a clustered thick disk-like co...</td>\n",
       "      <td>MOND is a theory that increases the discrepanc...</td>\n",
       "      <td>MOND explains the missing baryonic mass in gal...</td>\n",
       "      <td>MOND is a theory that reduces the observed mis...</td>\n",
       "      <td>MOND is a theory that eliminates the observed ...</td>\n",
       "      <td>MOND's impact on the observed missing baryonic...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>Woody Hartman is a retired American soccer goa...</td>\n",
       "      <td>Ray Montgomerie is a former footballer who pla...</td>\n",
       "      <td>Ray Montgomerie is a former footballer who pla...</td>\n",
       "      <td>Ray Montgomerie is a former footballer who pla...</td>\n",
       "      <td>Ray Montgomerie is a former footballer who pla...</td>\n",
       "      <td>Ray Montgomerie is a former footballer who pla...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  In relation to Eunice Fay McKenzie's career, w...   \n",
       "1  How does Modified Newtonian Dynamics (MOND) im...   \n",
       "2  Which of the following statements accurately d...   \n",
       "\n",
       "                                             context  \\\n",
       "0  Eunice Fay McKenzie (February 19, 1918 – April...   \n",
       "1  The presence of a clustered thick disk-like co...   \n",
       "2  Woody Hartman is a retired American soccer goa...   \n",
       "\n",
       "                                                   A  \\\n",
       "0  McKenzie showcased her singing talents in nume...   \n",
       "1  MOND is a theory that increases the discrepanc...   \n",
       "2  Ray Montgomerie is a former footballer who pla...   \n",
       "\n",
       "                                                   B  \\\n",
       "0  McKenzie is primarily remembered for her starr...   \n",
       "1  MOND explains the missing baryonic mass in gal...   \n",
       "2  Ray Montgomerie is a former footballer who pla...   \n",
       "\n",
       "                                                   C  \\\n",
       "0  McKenzie gained recognition for her role as a ...   \n",
       "1  MOND is a theory that reduces the observed mis...   \n",
       "2  Ray Montgomerie is a former footballer who pla...   \n",
       "\n",
       "                                                   D  \\\n",
       "0  McKenzie's collaborations with director Blake ...   \n",
       "1  MOND is a theory that eliminates the observed ...   \n",
       "2  Ray Montgomerie is a former footballer who pla...   \n",
       "\n",
       "                                                   E answer  \n",
       "0  McKenzie's successful career in sound films co...      B  \n",
       "1  MOND's impact on the observed missing baryonic...      E  \n",
       "2  Ray Montgomerie is a former footballer who pla...      B  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df = augmented_df.loc[~augmented_df.prompt.isin(df.prompt)].dropna()\n",
    "augmented_df = augmented_df.drop_duplicates(subset=\"prompt\")\n",
    "augmented_df = augmented_df[\n",
    "    [\n",
    "        \"prompt\",\n",
    "        \"context\",\n",
    "        \"A\",\n",
    "        \"B\",\n",
    "        \"C\",\n",
    "        \"D\",\n",
    "        \"E\",\n",
    "        \"answer\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Creating training and validation sets\n",
    "train_df = augmented_df\n",
    "val_df = df\n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "502fb813",
   "metadata": {
    "id": "502fb813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40619, 8)\n",
      "(0, 8)\n",
      "(40619, 8)\n"
     ]
    }
   ],
   "source": [
    "folding = FOLDING\n",
    "n_splits = N_SPLITS\n",
    "if folding:\n",
    "    # Crear una instancia de KFold\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, random_state=21, shuffle=True)\n",
    "\n",
    "    for fold, (train_indice, test_indice) in enumerate(kfold.split(augmented_df, augmented_df.answer)):\n",
    "        if fold == 3:\n",
    "            print(fold)\n",
    "\n",
    "            train_df = augmented_df.iloc[train_indice]\n",
    "print(train_df.shape)\n",
    "train_df = train_df.fillna(\"MASK_NAS\")\n",
    "train_df = train_df.loc[~train_df.prompt.isin(df.prompt)]\n",
    "\n",
    "print(train_df.loc[train_df.prompt.isin(df.prompt)].shape)\n",
    "train_df = train_df.drop_duplicates(subset=\"prompt\")\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98eacdbc",
   "metadata": {
    "id": "98eacdbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Dataset:\n",
      "\n",
      "Dataset({\n",
      "    features: ['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer', '__index_level_0__'],\n",
      "    num_rows: 100\n",
      "})\n",
      "\n",
      "Validation Dataset:\n",
      "\n",
      "Dataset({\n",
      "    features: ['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Converting dataframes into datasets\n",
    "if USE_SAMPLE:\n",
    "    train_ds = Dataset.from_pandas(train_df.iloc[:100])\n",
    "else:\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_df[\"all\"] = train_df[\"prompt\"] + train_df[\"A\"] + train_df[\"B\"] + train_df[\"C\"] + train_df[\"D\"] + train_df[\"E\"]\n",
    "option_to_index, index_to_option = create_option_mappings()\n",
    "\n",
    "print(\"\\nTrain Dataset:\\n\")\n",
    "print(train_ds)\n",
    "print(\"\\nValidation Dataset:\\n\")\n",
    "print(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d26823a4",
   "metadata": {
    "id": "d26823a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMultipleChoice: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = MODEL_NAME \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Instantiating model,\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
    "model = model.to(device)  # GPU0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14FJpD6-O6-z",
   "metadata": {
    "id": "14FJpD6-O6-z"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(param.requires_grad)\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "kkNGKANbp7bV",
   "metadata": {
    "id": "kkNGKANbp7bV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing embeddings.\n",
      "Freezing 18 layers.\n"
     ]
    }
   ],
   "source": [
    "## FREZZE EMBEDDINGS TO SPEED UP AND IMPROVE FINETUNNING\n",
    "if FREEZE_EMBEDDINGS:\n",
    "    print(\"Freezing embeddings.\")\n",
    "    for param in model.deberta.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "if FREEZE_LAYERS > 0:\n",
    "    print(f\"Freezing {FREEZE_LAYERS} layers.\")\n",
    "    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "TEAonVN8mejn",
   "metadata": {
    "id": "TEAonVN8mejn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:04<00:00, 20.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_dir = MODEL_DIR # Directory to save model and files\n",
    "output_dir = model_dir\n",
    "checkpoint_dir = CHECKPOINT_DIR\n",
    "remove_cols=REMOVE_COLS \n",
    "# Tokenizing train Dataset\n",
    "r_cols = [e for e in  remove_cols  if e in train_ds.features ]\n",
    "tokenized_train_ds = train_ds.map(preprocess_wrapper(tokenizer,  max_input=MAX_INPUT_TRAIN), batched=False, remove_columns=r_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gu5WEnvZrfse",
   "metadata": {
    "id": "gu5WEnvZrfse"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:07<00:00, 27.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Model finetunning\n",
    "r_cols = [e for e in remove_cols if e in val_ds.features ]\n",
    "tokenized_val_ds = val_ds.map(preprocess_wrapper(tokenizer,max_input=MAX_INPUT_VAL), batched=False, remove_columns=r_cols)\n",
    "\n",
    "# Defining parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=checkpoint_dir,\n",
    "    warmup_ratio=0.8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=5,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    # gradient_accumulation_steps = 2,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=20,\n",
    "    # weight_decay=0.01,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "# Defining Trainer to train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.01)],\n",
    ")\n",
    "\n",
    "\n",
    "print(trainer.evaluate())\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Sy5f0gEXwal",
   "metadata": {
    "id": "2Sy5f0gEXwal"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('finetuned_deberta\\\\tokenizer_config.json',\n",
       " 'finetuned_deberta\\\\special_tokens_map.json',\n",
       " 'finetuned_deberta\\\\spm.model',\n",
       " 'finetuned_deberta\\\\added_tokens.json',\n",
       " 'finetuned_deberta\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(model_dir)\n",
    "# Save the tokenizer as well\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kagle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19544.443325,
   "end_time": "2023-08-31T01:49:04.686324",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-30T20:23:20.242999",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
