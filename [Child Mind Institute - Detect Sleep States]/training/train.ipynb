{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN4rPyB7AvhP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"../../\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.losses import loss_function,metrics\n",
        "from src.callbacks import MetricsCallback,CustomSchedule,ReduceLROnThreshold\n",
        "from src.models import Encoder,Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F1fCkzvAogT",
        "outputId": "ad6728cf-49f2-464f-b120-dd5c56bd6fd3"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "import logging\n",
        "import polars as pl\n",
        "logger=logging.getLogger()\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "logger.setLevel(logging.INFO)\n",
        "from tqdm import tqdm\n",
        "print(tf.__version__)\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "if tf.test.is_gpu_available():\n",
        "    print(\"TensorFlow está utilizando una GPU.\")\n",
        "    print(\"GPU utilizada:\", tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"TensorFlow no está utilizando una GPU.\")\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import joblib\n",
        "import gc\n",
        "import pickle as pkl\n",
        "from itertools import groupby"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.training import create_datasets,prepare_validation_data,get_inds,read_indices,read_val_indices,reshape_features,create_datasets\n",
        "from src.utils import truncate_days,sample_normalize,drop_initial_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.constants import TARGET,TEST_IDS,SIGMA,DATA_PATH\n",
        "from src.params import (\n",
        "    LEARNING_RATE,\n",
        "    STEPS_PER_EPOCH,\n",
        "    NUM_EPOCHS,\n",
        "    WARMUP_STEPS,\n",
        "    GPU_BATCH_SIZE,\n",
        "    SAMPLE_NORMALIZE,\n",
        "    DROP_INITIAL_DATE,\n",
        "    OPTMIZER_BETA1,\n",
        "    OPTMIZER_BETA2,\n",
        "    OPTMIZER_EPSILON,\n",
        "    ONLY_TEST,\n",
        "    FOLD,\n",
        "    TRAIN,\n",
        "    PREDICT,\n",
        "    CFG,\n",
        "    DIM,\n",
        "    TAM_CONSIDER \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC1spq9oAogU"
      },
      "outputs": [],
      "source": [
        "\n",
        "features=pkl.load(open(os.path.join(DATA_PATH,\"features.pkl\"), \"rb\"))\n",
        "targets=pkl.load(open(os.path.join(DATA_PATH,\"targets.pkl\"), \"rb\"))\n",
        "steps_=pkl.load(open(os.path.join(DATA_PATH,\"steps.pkl\"), \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "events = pd.read_csv(os.path.join(DATA_PATH,f\"train_events.csv\"))\n",
        "events_check = pd.read_csv(os. path.join (DATA_PATH,f\"train_events.csv\")).dropna(\n",
        "    subset=\"timestamp\"\n",
        ")\n",
        "data = pd.read_parquet(\n",
        "    os.path.join(DATA_PATH,f\"data_train.parquet\"),\n",
        "    columns=[\"series_id\", \"step\", \"anglez\", \"enmo\", \"minute\", \"sine\", \"cosine\", \"sleep\", \"timestamp\"],\n",
        ").iloc[:1000]\n",
        "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
        "\n",
        "## train ids are mapped to int values so we need a dict to map back in the final part to comparare with the events dataset\n",
        "dict_ids = pkl.load(open(f\"{DATA_PATH}/dict_ids.pkl\", \"rb\"))\n",
        "\n",
        "targets, _, ids = joblib.load(f\"{DATA_PATH}/train_data.pkl\")\n",
        "targets = {dict_ids[ids[i]]: targets[i] for i in range(len(targets))}\n",
        "\n",
        "ids_ = np.load(f\"{DATA_PATH}/ids.npy\", allow_pickle=True).reshape(-1)\n",
        "fold_ids = pkl.load(open(f\"{DATA_PATH}/folds_ids.pkl\", \"rb\"))\n",
        "ids_=list(dict_ids.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVlGjZY2phDC"
      },
      "outputs": [],
      "source": [
        "for i in range(len(features)):\n",
        "  for j in range(len(features[i])):\n",
        "    features[i][j] = features[i][j].reshape(-1,features[i][j].shape[1]*CFG[\"patch_size\"])\n",
        "    feats= features[i][j][:,[0,1,4,5,8,9,]]\n",
        "    sine_= features[i][j][:,[2,6,10,]].mean(axis=1).reshape(-1,1)\n",
        "    cosine_=features[i][j][:,[3,7,11,]].mean(axis=1).reshape(-1,1)\n",
        "    feats=np.concatenate([feats,sine_,cosine_],axis=1)\n",
        "    features[i][j]=feats\n",
        "    targets[i][j] =  targets[i][j].reshape(-1,CFG[\"patch_size\"],2).mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMs4qwJL4Qhi"
      },
      "outputs": [],
      "source": [
        "\n",
        "file = f'{DATA_PATH}/Zzzs_train_multi.parquet'\n",
        "series_id  = pd.read_parquet(file, columns=['series_id'])\n",
        "series_id = series_id.series_id.unique()\n",
        "swaped_dict={val:key for key,val in dict_ids.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU2MX2Ko_ySq"
      },
      "outputs": [],
      "source": [
        "id_=0\n",
        "j=0\n",
        "df_=data.loc[data.series_id==id_]\n",
        "df_=truncate_days(df_,id_, events_check, dict_ids)\n",
        "df_=drop_initial_date( df_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VulQnoX5-9Nq",
        "outputId": "7d28adc7-017b-4c64-bc27-179f4c3dcea2"
      },
      "outputs": [],
      "source": [
        "#### training\n",
        "for fold in range(3,4):\n",
        "  # use just one fold for trainin. only this fold is used to train the final model to submit to competition, so improving here is a must.\n",
        "  train_ids=[e for e in fold_ids[fold][0] if e not in TEST_IDS]\n",
        "  val_ids=[e for e in fold_ids[fold][1] if e not in TEST_IDS]+TEST_IDS\n",
        "  inds,val_inds,arr= get_inds(train_ids,val_ids,features=features)\n",
        "  len_val= len(val_inds)\n",
        "  remainder_val=(GPU_BATCH_SIZE - len(val_inds)%GPU_BATCH_SIZE)%GPU_BATCH_SIZE\n",
        "  print(f\"Remainder validation {remainder_val}\")\n",
        "  for i in range(remainder_val):\n",
        "      val_inds.append(val_inds[-1])\n",
        "\n",
        "  dataset =tf.data.Dataset.from_tensor_slices(dict(pd.DataFrame(inds)))\n",
        "  dataset = dataset .map(read_indices).batch( GPU_BATCH_SIZE, drop_remainder=False)\n",
        "\n",
        "\n",
        "  val_dataset =tf.data.Dataset.from_tensor_slices(dict(pd.DataFrame(val_inds)))\n",
        "\n",
        "  val_dataset = val_dataset .map(read_val_indices).batch( GPU_BATCH_SIZE, drop_remainder=False)\n",
        "\n",
        "\n",
        "  ## same as val_dataset but with batch size 1 to prediction\n",
        "  val_dataset_backup=tf.data.Dataset.from_tensor_slices(dict(pd.DataFrame(val_inds[:len_val])))\n",
        "  val_dataset__=val_dataset_backup .map(read_val_indices).batch( 1, drop_remainder=False)\n",
        "\n",
        "\n",
        "\n",
        "  model=Model()\n",
        "  scheduler=CustomSchedule(LEARNING_RATE, WARMUP_STEPS)\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=scheduler, beta_1=OPTMIZER_BETA1, beta_2=OPTMIZER_BETA2, epsilon=OPTMIZER_EPSILON)\n",
        "  model.build(input_shape=(GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'],DIM ))\n",
        "\n",
        "  model.compile(optimizer=optimizer,loss=loss_function)\n",
        "  #model.  summary()\n",
        "\n",
        "  model.fit(dataset,epochs= 10,steps_per_epoch=500,validation_data=val_dataset, callbacks=[MetricsCallback(FOLD=fold),])\n",
        "  model.save_weights(\"my_model_weights\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "kagle",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
